# Convolutional_Neural_Network
This is the second assignment project I did for the course Deep Learning I studied. I will summarize key and basic knowledge of Convolutional Neural Network.
Convolutional neural networks (aka convnets, CNN) is the most widely used model in computer vision applications. In my first assignment project of Neural Network, I have done densely connected networks. They are very effective as task layers when the input dimension is relatively small. But in computer vision applications, the inputs are images. In such applications, convnets achieve much better accuracy than densely connected networks. Convnets replaces densely connected layers with the convolution layers, which aim to capture spatial patterns in an image. When multiple convolutional layers are stacked together, they are able to capture spatial features of increasing levels of abstraction.

![image](https://github.com/user-attachments/assets/490f10a5-6d91-4b4f-b0a4-328cfc136912)

## Typical convnet architecture
Like other networks, a convnet also has three components: preprocessing layers, feature layers, and task layers.
![image](https://github.com/user-attachments/assets/b669b6f6-2c56-4a63-860a-6ef2a055826e)

For feature layers, we shall study two new layer types:

- convolution layers
- pooling (subsampling) layers

## Two properties of convnets
Convnets are designed to work with image data, with the following two properties:

- They can learn spatial hierarchies of patterns, meaning that they assemble more complex patterns using simpler patterns
- The patterns they learn are translation invariant, meaning that absolute location of an object in an image does not affect the network decision.

The visual world is fundamentally translation-invariant. A pattern learned can appear at different location of a picture.

A convolution slides a filter over the input feature map, stopping at every possible location to capture local translation-invariant features, achieving:

- filter parameter sharing
- data efficiency on perceptual problems
- avoiding custom feature engineering

Here is a typical convolutional network definition in Keras.
```python
model = tf.keras.models.Sequential()

## Convolutional base
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))

## Dense head
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10))



model.summary()
```
## Convolutional layers
 A densely connected layer is nothing more than an affine transformation of the input tensor followed by an activation function


Like the linear operator above, convolution kernel (filter) is also a linear transformation. However, instead of being applied on the complete input 
, it is applied on local patches in 
![image](https://github.com/user-attachments/assets/f8c5e648-12f3-4551-95cc-2dc3aa91a6ea)

Convolution reduces the number of parameters. An image is often too big to have a different weight for each pixel. Convolution kernel is applied to all local patches with the same set of weights. The number of tunable weights does not depend on the image size. This is known as parameter sharing. Sometimes it is also called sparse connection, to emphasise the distinction from densely connected layers. In the illustration below, only one shared kernel is used. The number of parameters depends only on:

- the input patch size (3x3 here)
- the number of input channel (2 here)
- the number of output channel (3 here)

  ![image](https://github.com/user-attachments/assets/12be400e-ef20-4b31-b339-a838db2b3e38)
  ![image](https://github.com/user-attachments/assets/d2da51b4-5600-41bb-affd-968600571f44)

The output of a convolution layer is often called a feature map. A feature map consists of multiple channels. Each channel is like a processed image, indicating the likely locations of one feature in the input data.
## The max-pooling operation
When we go up along the hierarchy, a typical pattern will cover a wider area. Does that mean we need wider and wider convolution kernels? Not really. With convolution kernels stacked on top of each other, the feature generated by a kernel will gradually cover a wider area. The area is known as the effective receptive field.

However, if we uses only the standard convolution kernels, the effective receptive field grows too slowly with the number of added layers. That means that we need many convolution layers to cover sufficiently large receptive field.

To overcome this problem, we need to also downsample the feature maps. There are two operations to achieve this:

- Set a stride greater than 1 for the convolution kernel
- Add a pooling layer over the feature map
Convolution strides
Stride is a parameter that defines the distance between two successive windows when applying convolution. The following figure shows four successive windows of a 3x3 kernel. You may see a skip between two windows.
![image](https://github.com/user-attachments/assets/2168f7cc-2dbb-4ff9-a55c-a17acbc169d6)
The max-pooling operation
Most commonly, a max-pooling (or average pooling) layer is used to downsample feature maps and increase the effective receptive field. As the name suggests, max-pooling outputs the max value of each channel in a window of the input feature map.

Max-pooling can be viewed as a special type of convolution (with the nonlinear max tensor operation)
![image](https://github.com/user-attachments/assets/01604bdd-c97f-4750-9abc-5867d076ec99)
In Keras, you may specify the pool size the same way that you specify the kernel size of the convolution layer.
```python
tf.keras.layers.MaxPooling2D(pool_size=(2, 2))
```


